{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>intent</th>\n",
       "      <th>ilabels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>\"Feeling Overwhelmed by the Dreadful 'Sense o...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnxietyandStressDisorders</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>\"Desperately Seeking Relief from the Constant...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnxietyandStressDisorders</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>\"Feeling Overwhelmed by 'Doom' and Health Anx...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnxietyandStressDisorders</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>assistance</td>\n",
       "      <td>Title - Seeking STEM Students' Assistance in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>HousingSupportandAssistance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>assistance</td>\n",
       "      <td>Title - Calling all STEM students for valuabl...</td>\n",
       "      <td>0</td>\n",
       "      <td>HousingSupportandAssistance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11347</th>\n",
       "      <td>relationships</td>\n",
       "      <td>* Her, a week ago: Precious, how are you? (I i...</td>\n",
       "      <td>0</td>\n",
       "      <td>RelationshipsandAbuse</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11348</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>I don't have the ability to cope with it anymo...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnxietyandStressDisorders</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11349</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>In case this is the first time you're reading ...</td>\n",
       "      <td>0</td>\n",
       "      <td>AnxietyandStressDisorders</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11350</th>\n",
       "      <td>almosthomeless</td>\n",
       "      <td>Do you find this normal? They have a good rela...</td>\n",
       "      <td>0</td>\n",
       "      <td>HousingSupportandAssistance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11351</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>I was talking to my mom this morning and she s...</td>\n",
       "      <td>1</td>\n",
       "      <td>AnxietyandStressDisorders</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11352 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            subreddit                                               text  \\\n",
       "0                ptsd   \"Feeling Overwhelmed by the Dreadful 'Sense o...   \n",
       "1                ptsd   \"Desperately Seeking Relief from the Constant...   \n",
       "2                ptsd   \"Feeling Overwhelmed by 'Doom' and Health Anx...   \n",
       "3          assistance   Title - Seeking STEM Students' Assistance in ...   \n",
       "4          assistance   Title - Calling all STEM students for valuabl...   \n",
       "...               ...                                                ...   \n",
       "11347   relationships  * Her, a week ago: Precious, how are you? (I i...   \n",
       "11348            ptsd  I don't have the ability to cope with it anymo...   \n",
       "11349         anxiety  In case this is the first time you're reading ...   \n",
       "11350  almosthomeless  Do you find this normal? They have a good rela...   \n",
       "11351            ptsd  I was talking to my mom this morning and she s...   \n",
       "\n",
       "       label                       intent  ilabels  \n",
       "0          1    AnxietyandStressDisorders        0  \n",
       "1          1    AnxietyandStressDisorders        0  \n",
       "2          1    AnxietyandStressDisorders        0  \n",
       "3          0  HousingSupportandAssistance        1  \n",
       "4          0  HousingSupportandAssistance        1  \n",
       "...      ...                          ...      ...  \n",
       "11347      0        RelationshipsandAbuse        2  \n",
       "11348      1    AnxietyandStressDisorders        0  \n",
       "11349      0    AnxietyandStressDisorders        0  \n",
       "11350      0  HousingSupportandAssistance        1  \n",
       "11351      1    AnxietyandStressDisorders        0  \n",
       "\n",
       "[11352 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "augmented_data = pd.read_csv(\"/home/irteam/minhah-dcloud-dir/minhah/dreaddit/dreaddit_combined_data_gpt_more_11352_original.csv\")\n",
    "# \n",
    "augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irteam/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Load original and augmented data from CSV files\n",
    "original_data = pd.read_csv(\"/home/irteam/minhah-dcloud-dir/minhah/dreaddit/dreaddit-train-pp.csv\")\n",
    "# augmented_data = pd.read_csv(\"/home/irteam/minhah-dcloud-dir/minhah/dreaddit/dreaddit-train-pp.csv\")\n",
    "\n",
    "# Merge the two dataframes with an outer join using `concat`.\n",
    "merged_df = pd.concat([original_data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Identify and keep only the non-duplicate rows from the merged dataframe.\n",
    "augmented_data = merged_df.drop_duplicates(keep=False)\n",
    "\n",
    "# Reset the index of the resulting dataframe.\n",
    "augmented_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Create lists to store tokenized text\n",
    "original_texts = original_data['text'].tolist()\n",
    "augmented_texts = augmented_data['text'].tolist()\n",
    "\n",
    "# Tokenize and convert to lowercase for original texts\n",
    "original_texts = [nltk.word_tokenize(text.lower()) for text in original_texts]\n",
    "\n",
    "# Tokenize and convert to lowercase for augmented texts\n",
    "augmented_texts = [nltk.word_tokenize(str(text).lower()) for text in augmented_texts]\n",
    "\n",
    "# Compute BLEU score for each pair of original and augmented texts\n",
    "bleu_scores = []\n",
    "\n",
    "for original, augmented in zip(original_texts, augmented_texts):\n",
    "    score = sentence_bleu([original], augmented, weights=(0.5, 0.5))  # Using n=2 (bigrams)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "# Calculate the average BLEU score\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "print(f\"Average BLEU Score: {average_bleu_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 2.07\n",
      "P-value: 0.038410\n",
      "At least one comparison is statistically significant (reject the null hypothesis)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import stats\n",
    "augmented_data = pd.read_csv(\"/home/irteam/minhah-dcloud-dir/minhah/dreaddit/dreaddit_combined_data_gpt_more_11352_original.csv\")\n",
    "augmented_data\n",
    "# Load original and augmented data from CSV files\n",
    "original_data = pd.read_csv(\"/home/irteam/minhah-dcloud-dir/minhah/dreaddit/dreaddit-train-pp.csv\")\n",
    "# Merge the two dataframes with an outer join using `concat`.\n",
    "merged_df = pd.concat([original_data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Identify and keep only the non-duplicate rows from the merged dataframe.\n",
    "augmented_data = merged_df.drop_duplicates(keep=False)\n",
    "\n",
    "# Reset the index of the resulting dataframe.\n",
    "augmented_data.reset_index(drop=True, inplace=True)\n",
    "# Assuming you have a 'text' column in both datasets\n",
    "original_text = original_data['text']\n",
    "# print(augmented_data['text'])\n",
    "augmented_text = augmented_data['text'].astype(str)\n",
    "\n",
    "# Convert text data to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "original_tfidf = vectorizer.fit_transform(original_text)\n",
    "augmented_tfidf = vectorizer.transform(augmented_text)\n",
    "\n",
    "# Perform a t-test\n",
    "t_stat, p_value = stats.ttest_ind(original_tfidf.toarray(), augmented_tfidf.toarray())\n",
    "\n",
    "# Set your significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Print the t-statistic and p-value\n",
    "print(f'T-statistic: {t_stat[0]:.2f}')\n",
    "print(f'P-value: {p_value[0]:.6f}')\n",
    "\n",
    "# Check if any p-value is less than alpha\n",
    "if (p_value < alpha).any():\n",
    "    print(\"At least one comparison is statistically significant (reject the null hypothesis)\")\n",
    "else:\n",
    "    print(\"No comparison is statistically significant (fail to reject the null hypothesis)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depression",
   "language": "python",
   "name": "depression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
